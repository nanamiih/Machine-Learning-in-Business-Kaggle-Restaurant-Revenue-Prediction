{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6c06a4-6f8f-4a67-866a-96f1a4a26223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae5ea9d-a044-4bd8-acea-7d42c7766073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load datasets with optimized data types\n",
    "def load_data():\n",
    "    sales_dtypes = {\n",
    "        \"id\": \"category\", \"item_id\": \"category\", \"dept_id\": \"category\",\n",
    "        \"cat_id\": \"category\", \"store_id\": \"category\", \"state_id\": \"category\"\n",
    "    }\n",
    "    calendar_dtypes = {\n",
    "        \"event_name_1\": \"category\", \"event_type_1\": \"category\",\n",
    "        \"event_name_2\": \"category\", \"event_type_2\": \"category\"\n",
    "    }\n",
    "    prices_dtypes = {\n",
    "        \"store_id\": \"category\", \"item_id\": \"category\"\n",
    "    }\n",
    "\n",
    "    # Load datasets\n",
    "    sales_data = pd.read_csv(r\"C:\\Users\\user\\Downloads\\m5-forecasting-accuracy\\sales_train_validation.csv\", dtype=sales_dtypes)\n",
    "    calendar = pd.read_csv(r\"C:\\Users\\user\\Downloads\\m5-forecasting-accuracy\\calendar.csv\", dtype=calendar_dtypes)\n",
    "    prices = pd.read_csv(r\"C:\\Users\\user\\Downloads\\m5-forecasting-accuracy\\sell_prices.csv\", dtype=prices_dtypes)\n",
    "    return sales_data, calendar, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d5ee63-aedb-4598-9613-2564e634e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_in_chunks(sales_data, calendar, prices, chunk_size=10000):\n",
    "    calendar = calendar[[\"d\", \"wm_yr_wk\", \"date\"]]\n",
    "    prices = prices[[\"store_id\", \"item_id\", \"wm_yr_wk\", \"sell_price\"]]\n",
    "\n",
    "    processed_chunks = []\n",
    "\n",
    "    # Process in smaller chunks\n",
    "    for start_row in range(0, len(sales_data), chunk_size):\n",
    "        chunk = sales_data.iloc[start_row:start_row + chunk_size]\n",
    "\n",
    "        # Melt chunk\n",
    "        melted_chunk = pd.melt(chunk, id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "                               var_name=\"d\", value_name=\"sales\")\n",
    "\n",
    "        # Merge with calendar\n",
    "        merged_chunk = melted_chunk.merge(calendar, how=\"left\", on=\"d\")\n",
    "\n",
    "        # Merge with prices\n",
    "        merged_chunk = merged_chunk.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])\n",
    "\n",
    "        # Add time-based features\n",
    "        merged_chunk['date'] = pd.to_datetime(merged_chunk['date'])\n",
    "        merged_chunk['dayofweek'] = merged_chunk['date'].dt.dayofweek\n",
    "        merged_chunk['month'] = merged_chunk['date'].dt.month\n",
    "        merged_chunk['year'] = merged_chunk['date'].dt.year\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        merged_chunk = merged_chunk.drop([\"date\", \"wm_yr_wk\"], axis=1)\n",
    "\n",
    "        # Separate numeric and non-numeric columns\n",
    "        numeric_cols = merged_chunk.select_dtypes(include=['number']).columns\n",
    "        non_numeric_cols = merged_chunk.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "        # Convert categorical columns to object type temporarily\n",
    "        for col in non_numeric_cols:\n",
    "            if merged_chunk[col].dtype.name == \"category\":\n",
    "                merged_chunk[col] = merged_chunk[col].astype(\"object\")\n",
    "\n",
    "        # Fill missing values\n",
    "        merged_chunk[numeric_cols] = merged_chunk[numeric_cols].fillna(0)  # Fill numeric columns with 0\n",
    "        merged_chunk[non_numeric_cols] = merged_chunk[non_numeric_cols].fillna(\"Unknown\")  # Fill non-numeric with \"Unknown\"\n",
    "\n",
    "        # Convert object columns back to category type for memory efficiency\n",
    "        for col in non_numeric_cols:\n",
    "            if merged_chunk[col].dtype.name == \"object\":\n",
    "                merged_chunk[col] = merged_chunk[col].astype(\"category\")\n",
    "\n",
    "        # Append processed chunk\n",
    "        processed_chunks.append(merged_chunk)\n",
    "\n",
    "    # Combine all chunks into a single dataframe\n",
    "    processed_data = pd.concat(processed_chunks, ignore_index=True)\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab2ded4-582d-42ae-8b5f-5001fd4e9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "sales_data, calendar, prices = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c834e7c-38cd-4232-be72-56622ced76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data in chunks\n",
    "processed_data = preprocess_data_in_chunks(sales_data, calendar, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e19717-afb6-4767-b2eb-1789bac4f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = processed_data.drop([\"sales\"], axis=1)\n",
    "y = processed_data[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96ec56e-7ce9-4da2-b99a-526bbfa44b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y is a Series with numeric values\n",
    "y = y.squeeze()  # Ensure 1D structure\n",
    "y = pd.to_numeric(y, errors=\"coerce\").fillna(0)  # Ensure numeric type and handle missing values\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7bb6a-fb28-49f9-b2ef-4cdca48fcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include reduce_cardinality function here\n",
    "def reduce_cardinality(data, threshold=50):\n",
    "    for col in data.select_dtypes(include=[\"category\", \"object\"]).columns:\n",
    "        if data[col].nunique() > threshold:\n",
    "            top_categories = data[col].value_counts().nlargest(threshold).index\n",
    "            data[col] = data[col].apply(lambda x: x if x in top_categories else \"Other\")\n",
    "    return data\n",
    "\n",
    "# Step 1: Reduce cardinality for categorical columns\n",
    "X_train = reduce_cardinality(X_train.copy())\n",
    "X_test = reduce_cardinality(X_test.copy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f998d0-4910-48db-81a5-a863e67c095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sparse one-hot encoding\n",
    "def one_hot_encode_sparse(data, columns):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from scipy.sparse import hstack\n",
    "    \n",
    "    vectorizers = {}\n",
    "    sparse_matrices = []\n",
    "    \n",
    "    for col in columns:\n",
    "        vec = CountVectorizer(tokenizer=lambda x: [x], lowercase=False, binary=True)\n",
    "        sparse_matrix = vec.fit_transform(data[col].astype(str))\n",
    "        sparse_matrices.append(sparse_matrix)\n",
    "        vectorizers[col] = vec\n",
    "    \n",
    "    return hstack(sparse_matrices), vectorizers\n",
    "\n",
    "categorical_cols = X_train.select_dtypes(include=[\"category\", \"object\"]).columns\n",
    "X_train_sparse, train_vectorizers = one_hot_encode_sparse(X_train, categorical_cols)\n",
    "X_test_sparse = hstack([train_vectorizers[col].transform(X_test[col].astype(str)) for col in categorical_cols])\n",
    "\n",
    "# Step 3: Combine sparse one-hot with numeric columns\n",
    "numeric_cols = X_train.select_dtypes(include=[\"number\"]).columns\n",
    "X_train_numeric = X_train[numeric_cols].to_numpy()\n",
    "X_test_numeric = X_test[numeric_cols].to_numpy()\n",
    "\n",
    "X_train_final = hstack([X_train_numeric, X_train_sparse])\n",
    "X_test_final = hstack([X_test_numeric, X_test_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f60c585d-c8e5-4ade-932a-2c2f36ff90b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\user\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "631cb7c1-82e7-4223-a4a0-ed547701e02d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.82 GiB for an array with shape (11, 46661896) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 11\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     13\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_test, y_pred))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:2083\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2083\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   2084\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2086\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2087\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2088\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[0;32m   2089\u001b[0m     ):\n\u001b[0;32m   2090\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1046\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1044\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m   1048\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:12281\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12207\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12210\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12211\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12279\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mas_array()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1656\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1656\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interleave(dtype\u001b[38;5;241m=\u001b[39mdtype, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1689\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1689\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1691\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.82 GiB for an array with shape (11, 46661896) and data type object"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "# Train and evaluate models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "    \"SVR\": SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results[name] = rmse\n",
    "    print(f\"{name} RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5f2eb-1442-4397-aecd-eed9c277f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_data[\"sales\"].head())\n",
    "print(processed_data[\"sales\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d1985-3bfb-4843-b26f-f00e7822e6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057a3da-8e2c-49d4-bac2-7dc549e1ce17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
